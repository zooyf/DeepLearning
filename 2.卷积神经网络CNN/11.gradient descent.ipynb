{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ltr\"><div class=\"ureact-markdown--markdown--3IhZa ureact-markdown \"><h3 id=\"-\">到达谷底</h3>\n",
    "<p>现在，我将简单复习下梯度下降概念，使我们能够开始用 MiniFlow 训练我们的网络。注意，我们的目标是通过尽量减小代价，使我们的网络输出与目标值尽量接近。你可以将代价看做一座山，我们想要到达山底。</p>\n",
    "<p>想象你的模型参数表示为一个停在山顶的球。直观地来说，我们希望将球推下山。这样可以明白，但是我们讨论的是代价函数，如何知道哪条路是下山呢？ </p>\n",
    "<p>幸运的是，梯度下降正好给出了这一信息。 </p>\n",
    "<p>严格来说，梯度实际上指的是上坡，是<strong>最陡上升</strong>方向。但是，如果我们在此值前面加个负号，就得出了<strong>最陡下降</strong>方向，这正是我们需要的。</p>\n",
    "<p>稍后你将详细了解梯度，但是暂时可以将其看做数字向量。每个数字表示我们应该对照着调整神经网络中相应的权重或偏置的数量。按照梯度值调整所有的权重和偏置降低了网络的代价（或误差）。</p>\n",
    "<p>听明白了吗？</p>\n",
    "<p>好的！现在我们知道朝着哪个方向推球了。下一步是考虑用多大的推力，称之为<em>学习速度</em>，该名称比较恰当，因为该值确定了神经网络学习的快慢速度。 </p>\n",
    "<p>你可能希望设置非常大的学习速度，这一网络就能学的非常快，对吧？ </p>\n",
    "<p>要小心！如果该值太大，可能会迭代过度并最终偏离目标。呀！</p>\n",
    "</div></div><span></span></div><div class=\"index--instructor-notes-container--24U8Y shared--outer-container--3eppq\"><div class=\"index--instructor-notes--39nNE layout--content--3Smmq\"><div><noscript></noscript></div></div></div></div><div class=\"index--container--2OwOl\"><div class=\"index--atom--lmAIo layout--content--3Smmq\"><div class=\"image-atom--image-atom--1XDdu\"><div class=\"index--image-atom-content--YoZVu\"><div class=\"index--image-and-annotations-container--1o6QP\"><img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/c75210b5-3250-4884-9161-854265644b01\" alt=\"**收敛**。这是理想的行为。\" class=\"index--image--1wh9w\" style=\"width: 320px;\"></div><div class=\"index--caption--34paT\"><div class=\"ureact-markdown--markdown--3IhZa ureact-markdown \"><p><strong>收敛</strong>。这是理想的行为。</p>\n",
    "</div></div></div></div><span></span></div><div class=\"index--instructor-notes-container--24U8Y shared--outer-container--3eppq\"><div class=\"index--instructor-notes--39nNE layout--content--3Smmq\"><div><noscript></noscript></div></div></div></div><div class=\"index--container--2OwOl\"><div class=\"index--atom--lmAIo layout--content--3Smmq\"><div class=\"image-atom--image-atom--1XDdu\"><div class=\"index--image-atom-content--YoZVu\"><div class=\"index--image-and-annotations-container--1o6QP\"><img src=\"https://s3.cn-north-1.amazonaws.com.cn/u-img/3eae2c9b-6c0a-4bfb-8d2f-d17c44676792\" alt=\"**发散**。当学习速度过高时会出现这种情况。\" class=\"index--image--1wh9w\" style=\"width: 320px;\"></div><div class=\"index--caption--34paT\"><div class=\"ureact-markdown--markdown--3IhZa ureact-markdown \"><p><strong>发散</strong>。当学习速度过高时会出现这种情况。</p>\n",
    "</div></div></div></div><span></span></div><div class=\"index--instructor-notes-container--24U8Y shared--outer-container--3eppq\"><div class=\"index--instructor-notes--39nNE layout--content--3Smmq\"><div><noscript></noscript></div></div></div></div><div class=\"index--container--2OwOl\"><div class=\"index--atom--lmAIo layout--content--3Smmq\"><div class=\"ltr\"><div class=\"ureact-markdown--markdown--3IhZa ureact-markdown \"><p>那么，什么样的学习速度合适呢？</p>\n",
    "<p>我们只能做出猜测，根据以往经验，0.1 至 0.0001 范围的值效果最好。0.001 至 0.0001 范围的值很常见，因为 0.1 和 0.01 有时候过大。</p>\n",
    "<p>下面是梯度下降的公式（伪代码）：</p>\n",
    "<pre><code><span class=\"hljs-constant\">x</span> = x - learning_rate * gradient_of_x\n",
    "</code></pre><p><code>x</code> 是神经网络使用的参数（即单个权重或偏置）。</p>\n",
    "<p>我们将 <code>gradient_of_x</code>（上坡方向）与 <code>learning_rate</code>（推力）相乘，然后将 <code>x</code> 减去相乘结果，从而向下推动。</p>\n",
    "<p>太棒了！现在来做一道练习吧。</p>\n",
    "</div></div><span></span></div><div class=\"index--instructor-notes-container--24U8Y shared--outer-container--3eppq\"><div class=\"index--instructor-notes--39nNE layout--content--3Smmq\"><div><noscript></noscript></div></div></div></div><div class=\"index--container--2OwOl\"><div class=\"index--atom--lmAIo layout--content--3Smmq\"><div class=\"ltr\"><div class=\"ureact-markdown--markdown--3IhZa ureact-markdown \"><h3 id=\"-\">设置</h3>\n",
    "<p>对于这道测验，你将完成 <code>f.py</code> 和 <code>gd.py</code> 文件中的 <code>TODO</code>。</p>\n",
    "<p>任务：</p>\n",
    "<ul>\n",
    "<li>在 <code>f.py</code> 中设置 <code>learning_rate</code>。</li>\n",
    "<li>在 <code>gd.py</code> 的 <code>gradient_descent_update</code> 函数中完成梯度下降的实现代码。</li>\n",
    "</ul>\n",
    "<p>备注：</p>\n",
    "<ul>\n",
    "<li>如果你正确地实现了梯度下降代码，将 <code>learning_rate</code> 设为 0.1 应该得出 <code>x</code> -&gt; 0 及 <code>f(x)</code> -&gt; 5。</li>\n",
    "<li>将学习速度设为不同的值，看看效果如何。试试非常小的值、接近 1 的值、大于 1 的值，等等。会出现什么情况？</li>\n",
    "</ul>\n",
    "</div></div><span></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    \n",
    "    # Return the new value for x\n",
    "    return x-gradx*learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 41383494.000, x = 12866.000\n",
      "EPOCH 1: Cost = 26485437.960, x = 10292.800\n",
      "EPOCH 2: Cost = 16950682.094, x = 8234.240\n",
      "EPOCH 3: Cost = 10848438.340, x = 6587.392\n",
      "EPOCH 4: Cost = 6943002.338, x = 5269.914\n",
      "EPOCH 5: Cost = 4443523.296, x = 4215.931\n",
      "EPOCH 6: Cost = 2843856.710, x = 3372.745\n",
      "EPOCH 7: Cost = 1820070.094, x = 2698.196\n",
      "EPOCH 8: Cost = 1164846.660, x = 2158.557\n",
      "EPOCH 9: Cost = 745503.663, x = 1726.845\n",
      "EPOCH 10: Cost = 477124.144, x = 1381.476\n",
      "EPOCH 11: Cost = 305361.252, x = 1105.181\n",
      "EPOCH 12: Cost = 195433.001, x = 884.145\n",
      "EPOCH 13: Cost = 125078.921, x = 707.316\n",
      "EPOCH 14: Cost = 80052.309, x = 565.853\n",
      "EPOCH 15: Cost = 51235.278, x = 452.682\n",
      "EPOCH 16: Cost = 32792.378, x = 362.146\n",
      "EPOCH 17: Cost = 20988.922, x = 289.717\n",
      "EPOCH 18: Cost = 13434.710, x = 231.773\n",
      "EPOCH 19: Cost = 8600.014, x = 185.419\n",
      "EPOCH 20: Cost = 5505.809, x = 148.335\n",
      "EPOCH 21: Cost = 3525.518, x = 118.668\n",
      "EPOCH 22: Cost = 2258.131, x = 94.934\n",
      "EPOCH 23: Cost = 1447.004, x = 75.947\n",
      "EPOCH 24: Cost = 927.883, x = 60.758\n",
      "EPOCH 25: Cost = 595.645, x = 48.606\n",
      "EPOCH 26: Cost = 383.013, x = 38.885\n",
      "EPOCH 27: Cost = 246.928, x = 31.108\n",
      "EPOCH 28: Cost = 159.834, x = 24.886\n",
      "EPOCH 29: Cost = 104.094, x = 19.909\n",
      "EPOCH 30: Cost = 68.420, x = 15.927\n",
      "EPOCH 31: Cost = 45.589, x = 12.742\n",
      "EPOCH 32: Cost = 30.977, x = 10.193\n",
      "EPOCH 33: Cost = 21.625, x = 8.155\n",
      "EPOCH 34: Cost = 15.640, x = 6.524\n",
      "EPOCH 35: Cost = 11.810, x = 5.219\n",
      "EPOCH 36: Cost = 9.358, x = 4.175\n",
      "EPOCH 37: Cost = 7.789, x = 3.340\n",
      "EPOCH 38: Cost = 6.785, x = 2.672\n",
      "EPOCH 39: Cost = 6.142, x = 2.138\n",
      "EPOCH 40: Cost = 5.731, x = 1.710\n",
      "EPOCH 41: Cost = 5.468, x = 1.368\n",
      "EPOCH 42: Cost = 5.299, x = 1.095\n",
      "EPOCH 43: Cost = 5.192, x = 0.876\n",
      "EPOCH 44: Cost = 5.123, x = 0.700\n",
      "EPOCH 45: Cost = 5.079, x = 0.560\n",
      "EPOCH 46: Cost = 5.050, x = 0.448\n",
      "EPOCH 47: Cost = 5.032, x = 0.359\n",
      "EPOCH 48: Cost = 5.021, x = 0.287\n",
      "EPOCH 49: Cost = 5.013, x = 0.230\n",
      "EPOCH 50: Cost = 5.008, x = 0.184\n",
      "EPOCH 51: Cost = 5.005, x = 0.147\n",
      "EPOCH 52: Cost = 5.003, x = 0.118\n",
      "EPOCH 53: Cost = 5.002, x = 0.094\n",
      "EPOCH 54: Cost = 5.001, x = 0.075\n",
      "EPOCH 55: Cost = 5.001, x = 0.060\n",
      "EPOCH 56: Cost = 5.001, x = 0.048\n",
      "EPOCH 57: Cost = 5.000, x = 0.039\n",
      "EPOCH 58: Cost = 5.000, x = 0.031\n",
      "EPOCH 59: Cost = 5.000, x = 0.025\n",
      "EPOCH 60: Cost = 5.000, x = 0.020\n",
      "EPOCH 61: Cost = 5.000, x = 0.016\n",
      "EPOCH 62: Cost = 5.000, x = 0.013\n",
      "EPOCH 63: Cost = 5.000, x = 0.010\n",
      "EPOCH 64: Cost = 5.000, x = 0.008\n",
      "EPOCH 65: Cost = 5.000, x = 0.006\n",
      "EPOCH 66: Cost = 5.000, x = 0.005\n",
      "EPOCH 67: Cost = 5.000, x = 0.004\n",
      "EPOCH 68: Cost = 5.000, x = 0.003\n",
      "EPOCH 69: Cost = 5.000, x = 0.003\n",
      "EPOCH 70: Cost = 5.000, x = 0.002\n",
      "EPOCH 71: Cost = 5.000, x = 0.002\n",
      "EPOCH 72: Cost = 5.000, x = 0.001\n",
      "EPOCH 73: Cost = 5.000, x = 0.001\n",
      "EPOCH 74: Cost = 5.000, x = 0.001\n",
      "EPOCH 75: Cost = 5.000, x = 0.001\n",
      "EPOCH 76: Cost = 5.000, x = 0.001\n",
      "EPOCH 77: Cost = 5.000, x = 0.000\n",
      "EPOCH 78: Cost = 5.000, x = 0.000\n",
      "EPOCH 79: Cost = 5.000, x = 0.000\n",
      "EPOCH 80: Cost = 5.000, x = 0.000\n",
      "EPOCH 81: Cost = 5.000, x = 0.000\n",
      "EPOCH 82: Cost = 5.000, x = 0.000\n",
      "EPOCH 83: Cost = 5.000, x = 0.000\n",
      "EPOCH 84: Cost = 5.000, x = 0.000\n",
      "EPOCH 85: Cost = 5.000, x = 0.000\n",
      "EPOCH 86: Cost = 5.000, x = 0.000\n",
      "EPOCH 87: Cost = 5.000, x = 0.000\n",
      "EPOCH 88: Cost = 5.000, x = 0.000\n",
      "EPOCH 89: Cost = 5.000, x = 0.000\n",
      "EPOCH 90: Cost = 5.000, x = 0.000\n",
      "EPOCH 91: Cost = 5.000, x = 0.000\n",
      "EPOCH 92: Cost = 5.000, x = 0.000\n",
      "EPOCH 93: Cost = 5.000, x = 0.000\n",
      "EPOCH 94: Cost = 5.000, x = 0.000\n",
      "EPOCH 95: Cost = 5.000, x = 0.000\n",
      "EPOCH 96: Cost = 5.000, x = 0.000\n",
      "EPOCH 97: Cost = 5.000, x = 0.000\n",
      "EPOCH 98: Cost = 5.000, x = 0.000\n",
      "EPOCH 99: Cost = 5.000, x = 0.000\n",
      "EPOCH 100: Cost = 5.000, x = 0.000\n"
     ]
    }
   ],
   "source": [
    "# Random number between 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
